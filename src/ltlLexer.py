# Generated from ltl.g4 by ANTLR 4.13.2
from antlr4 import *
from io import StringIO
import re
import sys
if sys.version_info[1] > 5:
    from typing import TextIO
else:
    from typing.io import TextIO


def serializedATN():
    return [
        4,0,18,115,6,-1,2,0,7,0,2,1,7,1,2,2,7,2,2,3,7,3,2,4,7,4,2,5,7,5,
        2,6,7,6,2,7,7,7,2,8,7,8,2,9,7,9,2,10,7,10,2,11,7,11,2,12,7,12,2,
        13,7,13,2,14,7,14,2,15,7,15,2,16,7,16,2,17,7,17,1,0,1,0,1,1,1,1,
        1,2,1,2,1,2,1,2,1,2,1,2,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,1,3,
        1,3,1,4,1,4,1,5,1,5,1,5,1,5,1,5,1,5,1,5,1,5,1,5,1,5,1,5,1,6,1,6,
        1,7,1,7,1,7,1,7,1,7,1,7,1,7,1,8,1,8,1,9,1,9,1,10,1,10,1,11,1,11,
        1,11,1,11,1,11,1,11,1,12,1,12,1,12,1,13,1,13,1,13,1,13,1,14,1,14,
        1,15,1,15,1,16,4,16,105,8,16,11,16,12,16,106,1,17,4,17,110,8,17,
        11,17,12,17,111,1,17,1,17,0,0,18,1,1,3,2,5,3,7,4,9,5,11,6,13,7,15,
        8,17,9,19,10,21,11,23,12,25,13,27,14,29,15,31,16,33,17,35,18,1,0,
        2,2,0,48,57,97,122,3,0,9,10,13,13,32,32,116,0,1,1,0,0,0,0,3,1,0,
        0,0,0,5,1,0,0,0,0,7,1,0,0,0,0,9,1,0,0,0,0,11,1,0,0,0,0,13,1,0,0,
        0,0,15,1,0,0,0,0,17,1,0,0,0,0,19,1,0,0,0,0,21,1,0,0,0,0,23,1,0,0,
        0,0,25,1,0,0,0,0,27,1,0,0,0,0,29,1,0,0,0,0,31,1,0,0,0,0,33,1,0,0,
        0,0,35,1,0,0,0,1,37,1,0,0,0,3,39,1,0,0,0,5,41,1,0,0,0,7,47,1,0,0,
        0,9,58,1,0,0,0,11,60,1,0,0,0,13,71,1,0,0,0,15,73,1,0,0,0,17,80,1,
        0,0,0,19,82,1,0,0,0,21,84,1,0,0,0,23,86,1,0,0,0,25,92,1,0,0,0,27,
        95,1,0,0,0,29,99,1,0,0,0,31,101,1,0,0,0,33,104,1,0,0,0,35,109,1,
        0,0,0,37,38,5,33,0,0,38,2,1,0,0,0,39,40,5,88,0,0,40,4,1,0,0,0,41,
        42,5,65,0,0,42,43,5,70,0,0,43,44,5,84,0,0,44,45,5,69,0,0,45,46,5,
        82,0,0,46,6,1,0,0,0,47,48,5,78,0,0,48,49,5,69,0,0,49,50,5,88,0,0,
        50,51,5,84,0,0,51,52,5,95,0,0,52,53,5,83,0,0,53,54,5,84,0,0,54,55,
        5,65,0,0,55,56,5,84,0,0,56,57,5,69,0,0,57,8,1,0,0,0,58,59,5,70,0,
        0,59,10,1,0,0,0,60,61,5,69,0,0,61,62,5,86,0,0,62,63,5,69,0,0,63,
        64,5,78,0,0,64,65,5,84,0,0,65,66,5,85,0,0,66,67,5,65,0,0,67,68,5,
        76,0,0,68,69,5,76,0,0,69,70,5,89,0,0,70,12,1,0,0,0,71,72,5,71,0,
        0,72,14,1,0,0,0,73,74,5,65,0,0,74,75,5,76,0,0,75,76,5,87,0,0,76,
        77,5,65,0,0,77,78,5,89,0,0,78,79,5,83,0,0,79,16,1,0,0,0,80,81,5,
        38,0,0,81,18,1,0,0,0,82,83,5,124,0,0,83,20,1,0,0,0,84,85,5,85,0,
        0,85,22,1,0,0,0,86,87,5,85,0,0,87,88,5,78,0,0,88,89,5,84,0,0,89,
        90,5,73,0,0,90,91,5,76,0,0,91,24,1,0,0,0,92,93,5,45,0,0,93,94,5,
        62,0,0,94,26,1,0,0,0,95,96,5,60,0,0,96,97,5,45,0,0,97,98,5,62,0,
        0,98,28,1,0,0,0,99,100,5,40,0,0,100,30,1,0,0,0,101,102,5,41,0,0,
        102,32,1,0,0,0,103,105,7,0,0,0,104,103,1,0,0,0,105,106,1,0,0,0,106,
        104,1,0,0,0,106,107,1,0,0,0,107,34,1,0,0,0,108,110,7,1,0,0,109,108,
        1,0,0,0,110,111,1,0,0,0,111,109,1,0,0,0,111,112,1,0,0,0,112,113,
        1,0,0,0,113,114,6,17,0,0,114,36,1,0,0,0,3,0,106,111,1,6,0,0
    ]

class ltlLexer(Lexer):

    atn = ATNDeserializer().deserialize(serializedATN())

    decisionsToDFA = [ DFA(ds, i) for i, ds in enumerate(atn.decisionToState) ]

    T__0 = 1
    T__1 = 2
    T__2 = 3
    T__3 = 4
    T__4 = 5
    T__5 = 6
    T__6 = 7
    T__7 = 8
    T__8 = 9
    T__9 = 10
    T__10 = 11
    T__11 = 12
    T__12 = 13
    T__13 = 14
    T__14 = 15
    T__15 = 16
    ID = 17
    WS = 18

    channelNames = [ u"DEFAULT_TOKEN_CHANNEL", u"HIDDEN" ]

    modeNames = [ "DEFAULT_MODE" ]

    literalNames = [ "<INVALID>",
            "'!'", "'X'", "'AFTER'", "'NEXT_STATE'", "'F'", "'EVENTUALLY'", 
            "'G'", "'ALWAYS'", "'&'", "'|'", "'U'", "'UNTIL'", "'->'", "'<->'", 
            "'('", "')'" ]

    symbolicNames = [ "<INVALID>",
            "ID", "WS" ]

    ruleNames = [ "T__0", "T__1", "T__2", "T__3", "T__4", "T__5", "T__6", 
                  "T__7", "T__8", "T__9", "T__10", "T__11", "T__12", "T__13", 
                  "T__14", "T__15", "ID", "WS" ]

    grammarFileName = "ltl.g4"

    _keyword_map = {
        "after": "AFTER",
        "next_state": "NEXT_STATE",
        "eventually": "EVENTUALLY",
        "always": "ALWAYS",
        "until": "UNTIL",
    }

    _keyword_pattern = re.compile(r"\b(" + "|".join(_keyword_map.keys()) + r")\b", re.IGNORECASE)

    @classmethod
    def _normalize_keywords(cls, text: str) -> str:
        return cls._keyword_pattern.sub(lambda m: cls._keyword_map[m.group(1).lower()], text)

    def __init__(self, input=None, output:TextIO = sys.stdout):
        if isinstance(input, str):
            input = InputStream(self._normalize_keywords(input))
        elif input is not None and hasattr(input, "strdata"):
            input = InputStream(self._normalize_keywords(input.strdata))

        super().__init__(input, output)
        self.checkVersion("4.13.2")
        self._interp = LexerATNSimulator(self, self.atn, self.decisionsToDFA, PredictionContextCache())
        self._actions = None
        self._predicates = None


